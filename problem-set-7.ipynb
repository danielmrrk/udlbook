{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 7.4\n",
    "\n",
    "Calculate the derivative $∂ℓ_i/∂f[x_i,ϕ]$' for the least squares loss function:\n",
    "\n",
    "$$ℓ_i = (y_i - f[x_i,ϕ])^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{∂ℓ_i}{∂f[x_i,ϕ]} = -2 \\cdot (y_i - f[x_i,ϕ])$\n",
    "\n",
    "It is interesting to see that the constant -2 is irrelevant since, this only does not change where the gradient is 0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 7.5\n",
    "\n",
    "Calculate the derivative $∂ℓ_i/∂f[x_i,ϕ]$ for the binary classification loss function:  \n",
    "\n",
    "$$ℓ_i = - (y_i \\cdot \\log[sig[f[x_i,ϕ]]] + (1 - y_i) \\cdot \\log[1 - sig[f[x_i,ϕ]]])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{aligned}\n",
    "\\frac{∂sig}{∂z} = \\frac{exp[-z]}{(1 + exp[-z])^2} = \\frac{1}{(1 + exp[-z])} \\cdot \\frac{exp[-z]}{(1 + exp[-z])} = sig[z] \\cdot (1 - sig[z]) \\text{ since } \\\\\n",
    "\\frac{exp[-z]}{(1 + exp[-z])} = \\frac{1 + exp[-z]}{1 + exp[-z]} - \\frac{1}{1 + exp[-z]} = 1 - \\frac{1}{1 + exp[-z]} = 1 - sig[z] \\\\ \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{∂ℓ_i}{∂f[x_i,ϕ]} &= - \\left(\\frac{y_i}{sig[f[x_i,ϕ]]} - \\frac{1 - y_i}{1 - sig[f[x_i,ϕ]]}\\right) \\cdot sig[f[x_i,ϕ]] \\cdot (1 - sig[f[x_i,ϕ]]) \\\\\n",
    "&= \\left(-\\frac{y_i}{sig[f[x_i,ϕ]]} + \\frac{1 - y_i}{1 - sig[f[x_i,ϕ]]}\\right) \\cdot sig[f[x_i,ϕ]] \\cdot (1 - sig[f[x_i,ϕ]]) \\\\\n",
    "&= -y_i \\cdot (1 - sig[f[x_i,ϕ]]) + (1 - y_i) \\cdot sig[f[x_i,ϕ]] \\\\\n",
    "&= -y_i + y_i \\cdot sig[f[x_i,ϕ]] + sig[f[x_i,ϕ]] - y_i \\cdot sig[f[x_i,ϕ]] \\\\\n",
    "&= sig[f[x_i,ϕ]] - y_i = \\hat{y_i} - y_i\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 7.6\n",
    "\n",
    "Show that for $z = β + Ω \\cdot h$:\n",
    "\n",
    "$$\n",
    "\\frac{∂z}{∂h}\n",
    "= Ω^T ,\n",
    "$$\n",
    "where $∂z/∂h$ is a matrix containing the term $∂z_i/∂h_j$ in its ith column and j-th row. To do this,  \n",
    "first find an expression for the constituent elements $∂z_i/∂h_j$ , and then consider the form that\n",
    "the matrix $∂z/∂h$ must take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be confusing, but its important to understand that $\\frac{∂z}{∂h}$ is not given as the normal Jacobian-Matrix,  \n",
    "which would look something like this.  \n",
    "  \n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{pmatrix}\n",
    "    \\frac{∂z_1}{∂h_1} & \\frac{∂z_1}{∂h_2} & \\cdots & \\frac{∂z_1}{∂h_j} \\\\\n",
    "    \\frac{∂z_2}{∂h_1} & \\frac{∂z_2}{∂h_2} & \\cdots & \\frac{∂z_2}{∂h_j} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{∂z_i}{∂h_1} & \\frac{∂z_i}{∂h_2} & \\cdots & \\frac{∂z_i}{∂h_j} \\\\\n",
    "\\end{pmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "But since when you have many examples $l_i$ you want $\\frac{∂z}{∂h}$ to be in the form  $D_{example}$ x $D_{output}$ and since  \n",
    "you do a matrix multiplication where you calculate $\\frac{∂z}{∂h}$ x grad_output. You need it to be in the form:  \n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{pmatrix}\n",
    "    \\frac{∂z_1}{∂h_1} & \\frac{∂z_2}{∂h_1} & \\cdots & \\frac{∂z_i}{∂h_1} \\\\\n",
    "    \\frac{∂z_1}{∂h_2} & \\frac{∂z_2}{∂h_2} & \\cdots & \\frac{∂z_i}{∂h_2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{∂z_1}{∂h_j} & \\frac{∂z_2}{∂h_j} & \\cdots & \\frac{∂z_i}{∂h_j} \\\\\n",
    "\\end{pmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "But since $z_i = \\beta_i + \\sum_{j=1}^J w_{ij} \\cdot h_j$ and $\\frac{∂z_i}{∂h_j} = w_{ij}$, but since the Jacobi Matrix is being  \n",
    "transposed the weight matrix needs also to be transposed. Normally $\\omega$ is $D_\\text{input dim}$$ x $D_{output_dim}$, but here you need,  \n",
    "$D_{output_dim}$ x $D_{input_dim}$, since you do a matrix calculation, $D_{examples}$ x $D_\\text{input dim}$, since you want to know, how  \n",
    "the loss changes if $h_1 \\text{ ... } h_j$ changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
